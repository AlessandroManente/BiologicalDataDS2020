{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "logical-melissa",
   "metadata": {},
   "source": [
    "# Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-quantity",
   "metadata": {},
   "source": [
    "## Part - 1 - Domain Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-punch",
   "metadata": {},
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-tiger",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import json\n",
    "import pandas as pd\n",
    "from code.interpro_data import *\n",
    "from code.HmmPy import *\n",
    "from code.metrics import *\n",
    "from Bio import SearchIO, SeqIO\n",
    "\n",
    "url = \"https://www.ebi.ac.uk/interpro/api/protein/reviewed/entry/pfam/pf03060?format=json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recreational-variety",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_domain = 'KTFEVRYPIIQAPMAGASTLELAATVTRLGGIGSIPMGSLSEKCDAIETQLENFDELVGDSGRIVNLNFFAHKEPRSGRADVNEEWLKKYDKIYGKAGIEFDKKELKLLYPSFRSIVDPQHPTVRLLKNLKPKIVSFHFGLPHEAVIESLQASDIKIFVTVTNLQEFQQAYESKLDGVVLQGWEAGGHRGNFKANDVEDGQLKTLDLVSTIVDYIDSASISNPPFIIAAGGIHDDESIKELLQFNIAAVQLGTVWLPSSQATISPEHLKMFQSPKSDTMMTAAISGRNLRTISTPFLRDLHQSSPLASIPDYPLPYDSFKSLANDAKQSGKGPQYSAFLAGSNYHKSWKDTRSTEEIFSILVQ'\n",
    "print(len(our_domain))\n",
    "print(our_domain[9:372])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-protection",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata, entries, gt = get_data(url, 1)\n",
    "print(gt.shape)\n",
    "gt.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-homework",
   "metadata": {},
   "source": [
    "Here we download with the InterPro APIs our ground truth for our domain. We get 23 sequences in the Interpro database that match our PFAM family."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-nothing",
   "metadata": {},
   "source": [
    "### Model Creation\n",
    "To build our models, our first task was to retrieve sequence homologues with our domain sequence.\n",
    "To do this, we performed both searches in reviewed and unreviewed databases, and we kept the significant hits (E-value < 0.01). We then made some attempts on varying the number of significant hits to see if that could improve the performances of the models.\n",
    "\n",
    "- In Swiss-Prot (annotated) we found 14 significant hits; (Call those try_1)\n",
    "- In Uniref90 we found 250 significant hits. (try_3)\n",
    "- From the 250 significant hits from Uniref90, we tried to take just the first 100 (try_4)\n",
    "\n",
    "For each of those sets of retrieved hits, we performed a multiple sequence alignment (MSA). To do this, made the MSAs using three different algorithms: T-Coffee, Muscle and Clustal-Omega. Comparing the results we found that T-Coffee is generally better.\n",
    "\n",
    "For each of those MSAs, we built a PSSM (Position-Specific Scoring Matrix) and a HMM (Hidden Markov Model). We then performed a PSI-BLAST search and HMM-Search using respectively our PSSM and our HMM as inputs. To do this we used the command line versions of *blast+* and *hmmer*; the bash scripts that manage the model creation can be found in data\\PSSMs\\pssm+psiblast.sh  data\\HMMSs\\generate_hmms.sh \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-glory",
   "metadata": {},
   "source": [
    "### Evaluate Ability of Matching sequences\n",
    "Once we got the hits from our models, using PSI-BLAST and HMMER searches, we parsed the output files inside tables, which contain, for each hit:\n",
    "- The Protein UniProt ID;\n",
    "- The relative E-value of the hit;\n",
    "- Where the model predicted the domain to begin;\n",
    "- Where the model predicted the domain to end;\n",
    "\n",
    "At this point, to evaluate the ability of our model to find the right sequences, we had to check how many sequences the model:\n",
    "- Correctly predicted to contain the domain (TP);\n",
    "- Wrongly predicted to contain the domain (FP);\n",
    "- Correctly predicted **not** to contain the domain (TN);\n",
    "- Wrongly predicted **not** to contain the domain (FN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-sword",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the last 3 parameters are thresholds for the E-values. Our models will consider significant only sequences that have e-values under that threshold.\n",
    "\n",
    "# - the first one is the Sequence E-Value for HMMs;\n",
    "# - the second one is the Domain independent E-value for HMMs;\n",
    "# - the third one is the E-Value returned by the Psi-Blast searches made with our PSSM models.\n",
    "\n",
    "threshold_hmms_e_value = 0.01\n",
    "threshold_hmms_i_e_value = 0.01\n",
    "threshold_pssm_e_value = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caroline-cinema",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df, parsed_tblouts, parsed_domtblouts, parsed_psiblast = metrics_8(gt, threshold_hmms_e_value, threshold_hmms_i_e_value, threshold_pssm_e_value)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-short",
   "metadata": {},
   "source": [
    "Since we have an highly unbalanced dataset, some of those metrics will be useless to us. We restrict just on the useful metrics, that in this case are precision, MCC and F1-Score.\n",
    "Maybe the most important in our case is precision, since we found that our models thend to have almost perfect recall, but bad precision. So we sort the data by precision to see the best models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collected-liverpool",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_useful = metrics_df.loc[:,('n_hits','precision','recall', 'mcc','f1_score')]\n",
    "metrics_df_useful.sort_values(by='precision', axis=0, ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-neighborhood",
   "metadata": {},
   "source": [
    "We see a high number of hits, which bring our precision down, but the recall is always high, which suggests us that the model is finding almost all the ground truth proteins, but also finds some other proteins that are negative.\n",
    "We try to plce a even lower threshold on the E-value to see if the performances are better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moderate-victorian",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_hmms_e_value = 0.001\n",
    "threshold_hmms_i_e_value = 0.001\n",
    "threshold_pssm_e_value = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-stylus",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df2, parsed_tblouts2, parsed_domtblouts2, parsed_psiblast2 = metrics_8(gt, threshold_hmms_e_value, threshold_hmms_i_e_value, threshold_pssm_e_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-richmond",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df2\n",
    "metrics_df2_useful = metrics_df2.loc[:,('n_hits','precision','recall', 'mcc','f1_score')]\n",
    "summary8 = metrics_df2_useful.sort_values(by='precision', axis=0, ascending=False).head(5)\n",
    "summary8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "communist-string",
   "metadata": {},
   "source": [
    "#### Plotting Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-difference",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(summary8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical-skating",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics_models(summary8, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "general-country",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics_summary(summary8, 8, threshold_hmms_e_value, threshold_hmms_i_e_value, threshold_pssm_e_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worth-impossible",
   "metadata": {},
   "source": [
    "### Evaluate the ability of matching domain positions\n",
    "Here we want to evaluate how good our model is at estimating domain positions inside the found proteins.\n",
    "\n",
    "We computed in a similar fashion, the values to obtain the metrics also in this case. \n",
    "\n",
    "Note that, technically, we should consider as True Negatives also all the proteins in SwissProt that our model correctly predicted not to have the domain (that is, all the proteins that our model didn't find, and that actually don't have the domain). While this is true, in this way any model would have an extremely high number of True Negatives, since the number of significant hits of a model is always much smaller than the size of uniprot (around 560.000 sequences). This statistic would not help us in any way to assess the ability of our models, and would significantly slow our computation times, so we decided not to compute that quantity.\n",
    "Below we report our results with some of the better performing models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungarian-premiere",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_hmms_e_value = 0.01\n",
    "threshold_hmms_i_e_value = 0.01\n",
    "threshold_pssm_e_value = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excellent-packaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df, conf_df = metrics_9(parsed_domtblouts, parsed_psiblast, gt, threshold_hmms_e_value, threshold_hmms_i_e_value, threshold_pssm_e_value)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pursuant-louis",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_useful = metrics_df.loc[:,('precision','recall', 'mcc','f1_score')]\n",
    "metrics_df_useful.sort_values(by='precision', axis=0, ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "british-better",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_hmms_e_value = 0.001\n",
    "threshold_hmms_i_e_value = 0.001\n",
    "threshold_pssm_e_value = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "central-nurse",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df2, conf_df2 = metrics_9(parsed_domtblouts, parsed_psiblast, gt, threshold_hmms_e_value, threshold_hmms_i_e_value, threshold_pssm_e_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-detail",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df2\n",
    "metrics_df2_useful = metrics_df2.loc[:,('precision','recall', 'mcc','f1_score')]\n",
    "summary = metrics_df2_useful.sort_values(by='precision', axis=0, ascending=False).head(5)\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-layout",
   "metadata": {},
   "source": [
    "#### Plotting Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sufficient-opening",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-tenant",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics_models(summary, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-explorer",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics_summary(summary, 9, threshold_hmms_e_value, threshold_hmms_i_e_value, threshold_pssm_e_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infectious-allocation",
   "metadata": {},
   "source": [
    "## Part - 2 - Domain Family Characterization\n",
    "From this point, we will have to choose a single best model. \n",
    "Since we will probably struggle to find the best model until the end, here the \"PATH_MODEL_PROTS\" will be the path of the csv file containing the prediction of the future best model.\n",
    "\n",
    "out_psiblast_M_4_denoised1_uniref90_1iterations.xml is the **final best model**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "played-currency",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import json\n",
    "import pandas as pd\n",
    "from code.interpro_data import *\n",
    "from code.HmmPy import *\n",
    "from code.metrics import *\n",
    "from Bio import SearchIO, SeqIO\n",
    "from Bio.PDB.PDBList import *\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-joshua",
   "metadata": {},
   "outputs": [],
   "source": [
    "from code.family_structures import *\n",
    "bestmodel = 'psiblast_M_4_denoised1_uniref90_1iterations'\n",
    "letter = bestmodel.split('_')[1]\n",
    "# original dataset\n",
    "PATH_MODEL_PROTS = path.join('data', 'part_1', 'PSSMs', 'PSSM_{}'.format(letter), 'parsed', 'out_{}.csv'.format(bestmodel)) #'.\\data_team_1\\PSSMs\\PSSM_C\\parsed\\out_{}.csv'.format(bestmodel)\n",
    "\n",
    "# map from pdb chains to uniprot entries\n",
    "# use your path where you saved the file for now\n",
    "# SIFTS_PATH = '..\\midterm exams\\midterm2\\data\\pdb_chain_uniprot.tsv'\n",
    "SIFTS_PATH = path.join('data', 'part_2', 'original_datasets', 'uniprot_segments_observed.tsv') #'data_team_1\\\\_part_2\\\\original_datasets\\\\uniprot_segments_observed.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breeding-vampire",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DONT DELETE THIS CELL. We use it to print the proteins found by the best model for ease of copy-pasting\n",
    "# the uniprot codes in https://www.uniprot.org/uploadlists/ to create \"family_sequences\" database!\n",
    "\n",
    "model_prots_df = pd.read_csv(PATH_MODEL_PROTS)\n",
    "model_prots = list(model_prots_df.ids.values)\n",
    "for i in model_prots[:5]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ongoing-filing",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_db = generate_pdb_df(SIFTS_PATH, PATH_MODEL_PROTS)\n",
    "# pdb_db.to_csv(\".\\\\data_team_1\\\\_part_2\\\\mappings\\\\mapping_{}.csv\".format(bestmodel))\n",
    "pdb_db.to_csv(path.join('data', 'part_2', 'mappings', 'mapping_{}.csv'.format(bestmodel)))\n",
    "pdb_db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generous-japanese",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only chains with 80% or more coverage\n",
    "pdb_db_filtered = filter_pdb_db(pdb_db)\n",
    "pdb_ids = list(pdb_db_filtered.pdb.unique())\n",
    "# print(pdb_ids)\n",
    "print(len(pdb_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cheap-curve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print for copy -paste into PDB website\n",
    "for i in pdb_ids:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-movie",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #We use the function \"retrieve_pdb_file\" provided by Biopython to automatically download all the needed pdb files.\n",
    "# pdblist = PDBList(server='ftp://ftp.wwpdb.org')\n",
    "# pdblist.download_pdb_files(pdb_codes = [code.upper() for code in pdb_ids], pdir = '.\\\\data_team_1\\\\_part_2\\\\original_datasets\\\\family_structures\\\\pdbs_{}'.format(bestmodel), file_format=\"pdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competitive-papua",
   "metadata": {},
   "source": [
    "### Structural Characterization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-hobby",
   "metadata": {},
   "source": [
    "#### Pairwise Structural Alignment between all the pdbs.\n",
    "\n",
    "**This must be made in linux**\n",
    "\n",
    "To make them in a procedural way we use the TMAlign command line program.\n",
    "Specifically, we generate all the .out files with this bash script:\n",
    "\n",
    "```\n",
    "echo \"Enter model type (psiblast or hmm)\"\n",
    "read model\n",
    "\n",
    "echo \"If PSIBLAST, how many iterations{\"\n",
    "read iterations\n",
    "\n",
    "\n",
    "echo \"Enter MSA method (C, M or O)\"\n",
    "read msamethod\n",
    "\n",
    "echo \"Enter try number\"\n",
    "read try\n",
    "\n",
    "echo \"Enter database (swissprot, uniref90, uniref50 or uniref100)\"\n",
    "read db\n",
    "\n",
    "if [ $model == 'psiblast' ]\n",
    "then\n",
    "\n",
    "directoryname=pdbs_${model}_${msamethod}_${try}_${db}_${iterations}iterations\n",
    "else\n",
    "directoryname=pdbs_${model}_${msamethod}_${try}_${db}\n",
    "fi\n",
    "\n",
    "for ent1 in ./${directoryname}/*.ent; do\n",
    "\tfor ent2 in ./${directoryname}/*.ent; do\n",
    "\t\t#echo \"${ent1}_${ent2}\"\n",
    "\t\tTMalign ${ent1} ${ent1} > ./temp/$(basename ${ent1})_$(basename ${ent2}).out\n",
    "\tdone\n",
    "done\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadly-ferry",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Once the .out files are done we need to parse them.\n",
    "# Since the generate .out files are a huge amount of files will result in a lot of used disk space, since all the info we need in those files is the TM-Scores and the RMSD, we will delete them after we finished to read them.\n",
    "rmsdmatrix = create_rmsd_matrix(bestmodel)\n",
    "rmsdmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "future-sword",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmscorematrix = create_tmscores_matrix(bestmodel)\n",
    "tmscorematrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coral-meeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE THE .ent FILES, NOW THAT WE FINISHED READING THEM\n",
    "clear_temp_folder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "english-skiing",
   "metadata": {},
   "source": [
    "#### Visualizing the Matrices\n",
    "Here we visualize both Matrices with the help of a heatmap, to help us spot any peculiar pattern or potential outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "headed-income",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csvs_path = \"./data_team_1/_part_2/original_datasets/family_structures/pdbs_{}/\".format(bestmodel)\n",
    "csvs_path = path.join('data', 'part_2', 'original_datasets', 'family_structures', 'pdbs_{}'.format(bestmodel))\n",
    "rmsd_filename = \"rmsds_{}\".format(bestmodel)\n",
    "tmscore_filename = \"tmscores_{}\".format(bestmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structured-essence",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap(csvs_path, rmsd_filename, header='RMSD Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-paris",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap(csvs_path, tmscore_filename, header='TM-Score Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-thread",
   "metadata": {},
   "source": [
    "#### Plot Dendograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-policy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_dendogram(rmsdmatrix, header = \"Hierarchical clustering using rmsd\", save_path = csvs_path + 'dendo_rmsd.png')\n",
    "plot_dendogram(rmsdmatrix, header = \"Hierarchical clustering using rmsd\", save_path = path.join(csvs_path, 'dendo_rmsd.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-economy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_dendogram(tmscorematrix, header = \"Hierarchical clustering using TM-Score\", save_path = csvs_path + 'dendo_tmscore.png' )\n",
    "plot_dendogram(tmscorematrix, header = \"Hierarchical clustering using TM-Score\", save_path = path.join(csvs_path, 'dendo_tmscore.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "altered-breed",
   "metadata": {},
   "source": [
    "#### Remove 2 outliers \n",
    "From the heatmaps and the dendograms we can easily spot two outliers.\n",
    "We Remove 6u8n and 6udq from the analisys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-beaver",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsdmatrix_2 = rmsdmatrix.drop(['6u8n','6udq'], axis = 0)\n",
    "rmsdmatrix_2 = rmsdmatrix_2.drop(['6u8n','6udq'], axis = 1)\n",
    "print(rmsdmatrix_2.shape)\n",
    "tmscorematrix_2 = tmscorematrix.drop(['6u8n','6udq'], axis = 0)\n",
    "tmscorematrix_2 = tmscorematrix_2.drop(['6u8n','6udq'], axis = 1)\n",
    "rmsdmatrix_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-architecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replot dendograms and heatmaps\n",
    "# rmsdmatrix_2.to_csv(\"./data_team_1/_part_2/original_datasets/family_structures/pdbs_{}/rmsds2_{}.csv\".format(bestmodel, bestmodel))\n",
    "rmsdmatrix_2.to_csv(path.join('data', 'part_2', 'original_datasets', 'family_structures', 'pdbs_{}'.format(bestmodel), 'rmsds2_{}.csv'.format(bestmodel)))\n",
    "\n",
    "# tmscorematrix_2.to_csv(\"./data_team_1/_part_2/original_datasets/family_structures/pdbs_{}/tmscores2_{}.csv\".format(bestmodel, bestmodel))\n",
    "tmscorematrix_2.to_csv(path.join('data', 'part_2', 'original_datasets', 'family_structures', 'pdbs_{}'.format(bestmodel), 'tmscores2_{}.csv'.format(bestmodel)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-coalition",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap(csvs_path, 'rmsds2_{}'.format(bestmodel), header='RMSD Matrix (No Outliers)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-marks",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap(csvs_path, 'tmscores2_{}'.format(bestmodel), header='TM-Scores Matrix (No Outliers)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terminal-subscriber",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_dendogram(rmsdmatrix_2, header = \"Hierarchical clustering using RMSD (No Outliers)\", save_path = csvs_path + 'dendo_rmsd_no_outliers.png')\n",
    "plot_dendogram(rmsdmatrix_2, header = \"Hierarchical clustering using RMSD (No Outliers)\", save_path = path.join(csvs_path, 'dendo_rmsd_no_outliers.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-material",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_dendogram(tmscorematrix_2, header = \"Hierarchical clustering using TM-Score (No Outliers)\", save_path = csvs_path + 'dendo_tmscore_no_outliers.png')\n",
    "plot_dendogram(tmscorematrix_2, header = \"Hierarchical clustering using TM-Score (No Outliers)\", save_path = path.join(csvs_path, 'dendo_tmscore_no_outliers.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-brick",
   "metadata": {},
   "source": [
    "#### mTM-Align\n",
    "We now perform multiple structural alignment on our *family_structures* dataset (without the outliers!) to identify and visualize conserved positions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colonial-booth",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.PDB import PDBList, is_aa, PDBIO\n",
    "from Bio.PDB.PDBParser import PDBParser\n",
    "from Bio.SeqUtils import IUPACData\n",
    "from Bio.PDB.PDBIO import Select\n",
    "from Bio.SeqIO.PdbIO import PdbSeqresIterator\n",
    "\n",
    "from Bio.PDB import PDBList, NeighborSearch\n",
    "from Bio.PDB.PDBParser import PDBParser\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-citation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_matrix(residues, threshold=False, seq_sep=6):\n",
    "\n",
    "    # Calculate the distance matrix\n",
    "    distances = []\n",
    "    for residue1 in residues:\n",
    "        if residue1.id[0] == \" \":  # Exclude hetero/water residues\n",
    "            row = []\n",
    "            for residue2 in residues:\n",
    "                if residue2.id[0] == \" \":  # Exclude hetero/water residues\n",
    "                    if abs(residue1.id[1] - residue2.id[1]) >= seq_sep:\n",
    "                        if threshold: #this way we can compute the distance matrices without \n",
    "                            #taking into account the threshold (so to answer question 5 \n",
    "                            # independentely of question 6)\n",
    "                            if (residue1[\"CA\"] - residue2[\"CA\"]) <= threshold:\n",
    "                                row.append(residue1[\"CA\"] - residue2[\"CA\"])\n",
    "                            else:\n",
    "                                row.append(None)\n",
    "                        else:\n",
    "                            row.append(residue1[\"CA\"] - residue2[\"CA\"])\n",
    "                    else:\n",
    "                        row.append(None)\n",
    "            distances.append(row)\n",
    "\n",
    "    return np.array(distances, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-delaware",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_struct = '.\\\\data_team_1\\\\_part_2\\\\original_datasets\\\\family_structures\\\\pdbs_{}\\\\superimposed_structure_core.pdb'.format(bestmodel)\n",
    "# path_struct_all = '.\\\\data_team_1\\\\_part_2\\\\original_datasets\\\\family_structures\\\\pdbs_{}\\\\superimposed_structure_all.pdb'.format(bestmodel)\n",
    "path_struct = path.join('data', 'part_2', 'original_datasets', 'family_structures', 'pdbs_{}'.format(bestmodel), 'superimposed_structure_core.pdb')\n",
    "path_struct_all = path.join('data', 'part_2', 'original_datasets', 'family_structures', 'pdbs_{}'.format(bestmodel), 'superimposed_structure_all.pdb')\n",
    "\n",
    "structure = PDBParser(QUIET=True).get_structure('core', path_struct)\n",
    "selected_residues = structure[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-indonesian",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_separation = 12\n",
    "threshold = False\n",
    "dist_matrix = [get_distance_matrix(residue, threshold, sequence_separation) for residue in selected_residues]\n",
    "# dist_matrix = get_distance_matrix(selected_residues['A'], threshold, sequence_separation) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bulgarian-edward",
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = 5\n",
    "nrows = int(np.ceil(len(dist_matrix) / ncols))\n",
    "\n",
    "plt.figure(figsize = (20,15))\n",
    "fig, ax = plt.subplots(nrows, ncols, sharex=True, sharey=True, figsize=(3*ncols, 2*nrows))\n",
    "plt.subplots_adjust(wspace=.05, hspace=.1)\n",
    "\n",
    "for i, dm in enumerate(dist_matrix):\n",
    "    sns.heatmap(dist_matrix[i], ax=ax[i//ncols, i%ncols], cmap='Blues')#, linewidths=.01, linecolor=\"black\")\n",
    "\n",
    "#plt.savefig(\"data_team_1\\\\_part_2\\\\original_datasets\\\\family_structures\\\\pdbs_{}\\\\dist_matrices_{}.pdf\".format(bestmodel, bestmodel), bbox_inches='tight')\n",
    "plt.savefig(path.join('data', 'part_2', 'original_datasets', 'family_structures', 'pdbs_{}'.format(bestmodel), 'dist_matrices_{}.pdf'.format(bestmodel)), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activated-abuse",
   "metadata": {},
   "source": [
    "#### Long Range Contacts\n",
    "In this section we want to visualize which residues are close in the structural sequence alignment but far (more than 12 residues apart) in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-warrant",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sequence_separation = 12\n",
    "threshold = 8\n",
    "contact_matrix = [get_distance_matrix(residue, threshold, sequence_separation) for residue in selected_residues]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-version",
   "metadata": {},
   "outputs": [],
   "source": [
    "contact_matrix = [np.nan_to_num(x, nan = 0) for x in contact_matrix]\n",
    "avg_contact_matrix = np.mean(contact_matrix, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "going-fundamental",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.heatmap(avg_contact_matrix, cmap='Blues')\n",
    "\n",
    "# plt.savefig(\"data_team_1\\\\_part_2\\\\original_datasets\\\\family_structures\\\\contact_map.pdf\", bbox_inches='tight')\n",
    "plt.savefig(path.join('data', 'part_2', 'original_datasets', 'family_structures', 'contact_map.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "circular-threshold",
   "metadata": {},
   "source": [
    "#### CATH Superfamily & Family\n",
    "Download file .tsv file here and place it in the original_dataset folder (I already added it to the .gitignore)\n",
    "ftp://ftp.ebi.ac.uk/pub/databases/msd/sifts/flatfiles/tsv/pdb_chain_cath_uniprot.tsv.gz\n",
    "\n",
    "We also use the cath_b.20201021.tsv file to map CATH_ids to CATH superfamilies.\n",
    "\n",
    "To make this point we will just need to process this .tsv similarly as we did to build our family_structures dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powerful-bronze",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all our proteins\n",
    "print(model_prots)\n",
    "print()\n",
    "print(len(model_prots))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-debate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uniprot_cath_path = '.\\\\data_team_1\\\\_part_2\\\\original_datasets\\\\pdb_chain_cath_uniprot.tsv'\n",
    "uniprot_cath_path = path.join('data', 'part_2', 'original_datasets', 'pdb_chain_cath_uniprot.tsv')\n",
    "uniprot_cath_map = pd.read_csv(uniprot_cath_path, sep = '\\t', header=1)\n",
    "uniprot_cath_map.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-template",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cath_superf_path = '.\\\\data_team_1\\\\_part_2\\\\original_datasets\\\\cath_b.20201021.tsv'\n",
    "cath_superf_path = path.join('data', 'part_2', 'original_datasets', 'cath_b.20201021.tsv')\n",
    "cath_superf_map = pd.read_csv(cath_superf_path, sep = ' ', header=None)\n",
    "cath_superf_map = cath_superf_map.drop(1, axis=1)\n",
    "cath_superf_map.columns = ['CATH_ID', 'SUPERF_ID', 'POS (?)']\n",
    "cath_superf_map.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-hardwood",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniprot_cath_map_model = uniprot_cath_map[uniprot_cath_map.SP_PRIMARY.isin(model_prots)].reset_index()\n",
    "uniprot_cath_map_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suffering-quantum",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_superfs = cath_superf_map[cath_superf_map.CATH_ID.isin(uniprot_cath_map_model.CATH_ID)].SUPERF_ID\n",
    "print(model_superfs.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-liability",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniprot_cath_map_model['CATH_SUPERFAMILY'] = model_superfs.values\n",
    "uniprot_cath_map_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alien-cleaners",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(uniprot_cath_map_model.CATH_SUPERFAMILY)\n",
    "plt.show()\n",
    "# http://www.cathdb.info/version/latest/superfamily/3.20.20.70"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-feeding",
   "metadata": {},
   "source": [
    "### Taxonomy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunrise-albuquerque",
   "metadata": {},
   "source": [
    "#### Retrieve Taxonomy Lineage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supreme-group",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import path\n",
    "import numpy as np\n",
    "from Bio import SeqIO, SearchIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-given",
   "metadata": {},
   "source": [
    "How to obtain family_seqs_tax_data:\n",
    "\n",
    "search all protein ids obtained with the best model on https://www.uniprot.org/uploadlists/ -> select FROM UniprotKB TO uniref90. then download .fasta format, parse it in order to obtain uniref ids -> remove \"UniRef90_\" chars and search all list on UniProt. Add \"tax_lineage\", \"tax_id\" columns and download .tsv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-progress",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = path.join('data', 'part_2', 'taxonomy', 'Uniprot_to_Uniref90.fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-halifax",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list(SeqIO.parse(p, \"fasta\"))\n",
    "for i in l:\n",
    "    print(i.id[9:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-console",
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_data_path = path.join('data', 'part_2', 'taxonomy', 'family_seqs_tax_data.tab')\n",
    "\n",
    "tax_data_db = pd.read_csv(tax_data_path, sep='\\t')\n",
    "\n",
    "tax_data_db = tax_data_db.drop(['Status','Organism ID', 'Entry name','Gene names', 'Length','Protein names'], axis=1)\n",
    "tax_data_db.columns = ['uniprot_id', 'organism', 'tax_id', 'tax_lineage']\n",
    "tax_data_db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-bonus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save TAX IDs to file\n",
    "np.savetxt(path.join('data', 'part_2', 'original_datasets', 'tax_ids_{}.txt'.format(bestmodel)) , X = tax_data_db.tax_id.values, fmt = '%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operating-devil",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new dict {key: P37527 ,value: 'cellular organisms',\n",
    "#                                        'Bacteria',\n",
    "#                                        'Terrabacteria group',\n",
    "#                                        'Firmicutes',\n",
    "#                                        'Bacilli',\n",
    "#                                        'Bacillales',\n",
    "#                                        'Bacillaceae',\n",
    "#                                        'Bacillus',\n",
    "#                                        'Bacillus subtilis group',\n",
    "#                                        'Bacillus subtilis',\n",
    "#                                        'Bacillus subtilis subsp. subtilis',\n",
    "#                                        'Bacillus subtilis (strain 168)'}\n",
    "lineage = {}\n",
    "for i in range(tax_data_db.shape[0]):\n",
    "    #print(tax_data_db[\"uniprot_id\"][i])\n",
    "    lineage.setdefault(tax_data_db[\"uniprot_id\"][i], tax_data_db[\"tax_lineage\"][i].split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-partnership",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show example\n",
    "lineage[\"P12269\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-forum",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create frequency dict (key: 'cellular organisms' , value :100)\n",
    "\n",
    "freq_lineage = {}\n",
    "for prot, tax in lineage.items():\n",
    "    for elem in tax:\n",
    "        freq_lineage[elem]=freq_lineage.setdefault(elem,0)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sophisticated-navigator",
   "metadata": {},
   "source": [
    "#### Plot Taxonomic Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-passing",
   "metadata": {},
   "source": [
    "Install ```ete3``` with ```pip``` but be aware that it may not install all the dependencies required for the package to run. To solve the issue locate the ```__init__.py``` of the library, change all the ```pass``` into ```raise``` and find out which module you are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaptive-tooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ete3 import Tree, TreeStyle, TextFace, NodeStyle, faces, AttrFace, CircleFace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-photograph",
   "metadata": {},
   "source": [
    "How to download phyliptree.phy:\n",
    "\n",
    "go to https://www.ncbi.nlm.nih.gov/Taxonomy/CommonTree/wwwcmt.cgi, and upload the previously saved .txt file (e.g. tax_ids_psiblast_M_4_denoised1_uniref90_1iterations.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "australian-guyana",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree=Tree(path.join('data', 'part_2', 'taxonomy', 'phyliptree.phy'),format=1,quoted_node_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "searching-namibia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layout(node):\n",
    "    if node.is_leaf():\n",
    "        # Add node name to leaf nodes\n",
    "        N = AttrFace(\"name\", fsize=35, fgcolor=\"black\")\n",
    "        faces.add_face_to_node(N, node, 0)\n",
    "    if \"weight\" in node.features:\n",
    "        # Creates a sphere face whose size is proportional to node's\n",
    "        # feature \"weight\"\n",
    "        C = CircleFace(radius=node.weight/2, color=\"Crimson\", style=\"circle\")\n",
    "        # Let's make the sphere transparent\n",
    "        C.opacity = 0.5\n",
    "        # And place as a float face over the tree\n",
    "        faces.add_face_to_node(C, node, 0, position=\"float\")\n",
    "\n",
    "for i, n in enumerate(tree.traverse()): #add freq to each node to show a abundance\n",
    "    w = [value for key,value in freq_lineage.items()][i]\n",
    "    n.add_features(weight=w)\n",
    "\n",
    "ts = TreeStyle()\n",
    "ts.title.add_face(TextFace(\"Taxonomic Tree\", fsize=90), column=0)\n",
    "ts.scale =  100\n",
    "ts.branch_vertical_margin = 10\n",
    "ts.layout_fn = layout\n",
    "\n",
    "# Draw a tree\n",
    "ts.mode = \"r\"\n",
    "\n",
    "# We will add node names manually\n",
    "ts.show_leaf_name = False\n",
    "# Show branch data\n",
    "ts.show_branch_length = False\n",
    "ts.show_branch_support = False\n",
    "#tree.show(tree_style=ts)\n",
    "tree.render(path.join('data', 'part_2', 'taxonomy', 'taxonomy_t_{}.png'.format(bestmodel)), w= 250 ,h=250, units=\"mm\",tree_style=ts,dpi=600);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automatic-relations",
   "metadata": {},
   "source": [
    "### Functional Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-zoning",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import copy\n",
    "from Bio import SeqIO, SearchIO\n",
    "from code import parse_go_obo\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from os import path\n",
    "\n",
    "# Paths of the gene ontology file, All GOAs for the Swissprot databas\n",
    "go_path = path.join('data', 'part_2', 'original_datasets', 'go.obo')\n",
    "goa_path = path.join('data', 'part_2', 'original_datasets', 'swissprot_goa_all.goa') \n",
    "family_sequences_path = path.join('data', 'part_2', 'original_datasets', 'family_sequences', 'family_seqs_{}.fasta'.format(bestmodel))\n",
    "\n",
    "family_sequences = list(SeqIO.parse(family_sequences_path, \"fasta\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharp-society",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I now parse the whole ontology file\n",
    "# dictionary with keys: proteins, items: ontology terms associated to the protein\n",
    "graph = parse_go_obo.parse_obo(go_path)\n",
    "ancestors, depth, roots = parse_go_obo.get_ancestors(graph)\n",
    "children = parse_go_obo.get_children(ancestors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocal-cinema",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataframe for the whole Swissprot GOA\n",
    "goa_db = pd.read_csv(goa_path, sep='\\t')\n",
    "goa_db = goa_db.loc[:,('Entry', 'Gene ontology IDs')]\n",
    "goa_db.columns = ['uniprot_id', 'go_terms']\n",
    "goa_db.go_terms =goa_db.go_terms.map(lambda x: x.split(\"; \"))\n",
    "goa_db['n_go_terms'] = goa_db.go_terms.map(lambda x: len(x))\n",
    "print('goa_db_shape: {}'.format(goa_db.shape))\n",
    "goa_db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-surrey",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "family_sequences_ids = [seq.id[9:] for seq in family_sequences]\n",
    "print('Number of proteins in our family_sequences database: {}'.format(len(family_sequences_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understood-cigarette",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now take only the proteins that are in our family_sequences \n",
    "goa_db_model = goa_db[goa_db.uniprot_id.isin(family_sequences_ids)]\n",
    "goa_db_model = goa_db_model.reset_index(drop=True)\n",
    "goa_db_model['n_go_terms'] = goa_db_model.go_terms.map(lambda x: len(x))\n",
    "print('goa_db_model shape: {}'.format(goa_db_model.shape))\n",
    "goa_db_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civil-horizontal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many repeated/unique, direct annotations in our dataset?\n",
    "repeated_goas_dataset = pd.Series(goa_db_model.go_terms.sum())\n",
    "unique_goas_dataset = repeated_goas_dataset.unique()\n",
    "\n",
    "print('Total number of GOAs: {}'.format(len(repeated_goas_dataset)))\n",
    "print('Number of unique GOAs: {}'.format(len(unique_goas_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-austin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many repeated/unique annotations in our dataset, after considering ancestors?\n",
    "# To see this we create a new dataframe where the go_terms will now include also all the ancestors of all the terms\n",
    "def include_ancestors(terms_list):\n",
    "    \"\"\"\n",
    "    Given a list of GO terms, returns a list of GO terms where all the ancestors of each term in terms_list is included. NB: the ancestors are added with repetition. \"\"\"\n",
    "    terms_list_ancestors = terms_list.copy()\n",
    "    for term in terms_list:\n",
    "        if term in ancestors.keys():\n",
    "            terms_list_ancestors.extend(list(ancestors[term]))\n",
    "    \n",
    "    return list(set(terms_list_ancestors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-ticket",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Database with goas of our model, including ancestors\n",
    "goa_db_model_anc = goa_db_model.copy()\n",
    "goa_db_model_anc.go_terms = goa_db_model.go_terms.map(lambda x: include_ancestors(x))\n",
    "goa_db_model_anc['n_go_terms'] = goa_db_model_anc.go_terms.map(lambda x: len(x))\n",
    "print(\"goa_db_model_anc shape: {}\".format(goa_db_model_anc.shape))\n",
    "goa_db_model_anc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-worse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # How many repeated/unique annotations in our dataset, after considering ancestors?\n",
    "repeated_goas_dataset_anc = pd.Series(goa_db_model_anc.go_terms.sum())\n",
    "unique_goas_dataset_anc = repeated_goas_dataset_anc.unique()\n",
    "\n",
    "print('Total number of GOAs: {}'.format(len(repeated_goas_dataset_anc)))\n",
    "print('Number of unique GOAs: {}'.format(len(unique_goas_dataset_anc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consistent-pulse",
   "metadata": {},
   "source": [
    "To find the enriched terms we have to methods:\n",
    "- Compute fold increase, and see the terms that have fold increase > 1;\n",
    "- Compute left and right p-values with fischer exact test and see the ones that have right-p value almost 0 and left p value almost 1;\n",
    "\n",
    "For both those two methods we will have to build a confusion matrix for each GO term, as defined below.\n",
    "\n",
    "The fold increase can be then calculated dividing the ratio having-/not-having the property of the selected with the ratio having-/not-having of the not selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "internal-circle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for each term must be like this.\n",
    "#                 |  Having the property      | Not having the property |\n",
    "# Selected        |           (1)             |         (2)             |\n",
    "# --------------- |---------------------------|-------------------------|\n",
    "# Not selected    |____________(3)____________|__________(4)____________|\n",
    "\n",
    "\n",
    "# 1 = N. of proteins with GO_i in our dataset;\n",
    "# 2 = N. of proteins without GO_i in our dataset;\n",
    "# 3 = N. of proteins with GO_i outside our dataset;\n",
    "# 4 = N. of proteins without GO_i ourtside our dataset;\n",
    "\n",
    "# Fold increase = (1/2) / (3/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-intention",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We create two dictionaries that will keep the count of how many proteins inside, and outside our dataset, have the GO annotation used as key.\n",
    "\n",
    "proteins_count_dataset_anc = {}  # { term : count } count within our dataset\n",
    "proteins_count_rest_anc = {}  # { term : count } count outsite our dataset\n",
    "\n",
    "# convert to dictionary our dataframe so we can iterate on it\n",
    "df_dict = goa_db.set_index('uniprot_id').to_dict()['go_terms']\n",
    "for j, (acc, annotations) in enumerate(df_dict.items()):\n",
    "    # print(acc)\n",
    "    # print(annotations)\n",
    "    terms1 = annotations.copy()\n",
    "    terms = include_ancestors(terms1)\n",
    "\n",
    "    # now populate the dictionaries\n",
    "    if acc in family_sequences_ids:\n",
    "        for term in terms:\n",
    "            proteins_count_dataset_anc.setdefault(term, 0)\n",
    "            proteins_count_dataset_anc[term] += 1\n",
    "    else:\n",
    "        for term in terms:\n",
    "            proteins_count_rest_anc.setdefault(term, 0)\n",
    "            proteins_count_rest_anc[term] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-combination",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of unique terms outside dataset, considering ancestors: {}\".format(len(proteins_count_rest_anc)))\n",
    "\n",
    "print(\"Number of unique terms inside dataset, considering ancestors: {}\".format(len(proteins_count_dataset_anc)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-roads",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prof solution\n",
    "data = []\n",
    "proteins_rest = goa_db.shape[0] - len(family_sequences_ids)\n",
    "\n",
    "for term in proteins_count_dataset_anc:\n",
    "    ratio_set = (proteins_count_dataset_anc[term] + 1) / (len(family_sequences_ids) -\n",
    "        proteins_count_dataset_anc[term] + 1)  # add pseudo count\n",
    "    ratio_rest = proteins_count_rest_anc.get(term, 1) / (proteins_rest -\n",
    "        proteins_count_rest_anc.get(term, 0) + 1)  # add pseudo count\n",
    "    fold_increase = ratio_set / ratio_rest\n",
    "    data.append((term, fold_increase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mexican-former",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-rebound",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filter = [item for item in data if item[1]>1]\n",
    "data_filter[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-tonight",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data))\n",
    "print(len(data_filter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-apache",
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_terms = [el[0] for el in data_filter]\n",
    "print(enriched_terms[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liable-agency",
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_terms_desc = [graph[enriched_term]['def'] for enriched_term in enriched_terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "herbal-minority",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = model_prots\n",
    "\n",
    "conf_tables = {} # {term : table}\n",
    "for term in proteins_count_dataset_anc.keys():\n",
    "    if term in proteins_count_dataset_anc.keys():\n",
    "        with_goi_dataset = proteins_count_dataset_anc[term]\n",
    "        without_goi_dataset = len(dataset) - with_goi_dataset\n",
    "    else:\n",
    "        with_goi_dataset = 0\n",
    "        without_goi_dataset  = len(dataset)\n",
    "    if term in proteins_count_rest_anc.keys():\n",
    "        with_goi_rest = proteins_count_rest_anc[term]\n",
    "        without_goi_rest = proteins_rest - with_goi_rest\n",
    "    else:\n",
    "        with_goi_rest = 0\n",
    "        without_goi_rest = proteins_rest\n",
    "\n",
    "    conf_tables.setdefault(term, [])\n",
    "    conf_tables[term] = np.array([[with_goi_dataset, without_goi_dataset], [with_goi_rest,without_goi_rest]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "musical-vulnerability",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "pvals = {} # {term : pvalues}\n",
    "for term in conf_tables:\n",
    "    conf_table = conf_tables[term]\n",
    "    _, twosided_p = stats.fisher_exact(conf_table)\n",
    "    _, left_p = stats.fisher_exact(conf_table, alternative='less')\n",
    "    _, right_p = stats.fisher_exact(conf_table, alternative='greater')\n",
    "    pvals.setdefault(term, [])\n",
    "    pvals[term] = [twosided_p, left_p, right_p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-maintenance",
   "metadata": {},
   "outputs": [],
   "source": [
    "pvals = pd.DataFrame.from_dict(pvals, columns=['twosided', 'left', 'right'], orient='index').reset_index()\n",
    "pvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upset-holder",
   "metadata": {},
   "outputs": [],
   "source": [
    "pvals_index = pvals['index'].to_list()\n",
    "data_dict = {}\n",
    "\n",
    "for dat in data:\n",
    "    data_dict[dat[0]] = dat[1]\n",
    "\n",
    "ordered_data = []\n",
    "\n",
    "for ind in pvals_index:\n",
    "    ordered_data.append(data_dict[ind])\n",
    "    \n",
    "pvals['fold_increase'] = ordered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-participation",
   "metadata": {},
   "outputs": [],
   "source": [
    "pvals_sorted = pvals.sort_values(by=['fold_increase'], ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-somerset",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_children = {}\n",
    "\n",
    "for i, (k, v) in enumerate(children.items()):\n",
    "    if k in pvals_index:\n",
    "        n_children[k] = len(v)\n",
    "\n",
    "pvals_sorted['n_children'] = pvals_sorted['index'].apply(lambda x: n_children[x] if x in list(n_children.keys()) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "speaking-paint",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = {}\n",
    "\n",
    "for i, (k, v) in enumerate(graph.items()):\n",
    "    if k in pvals_index:\n",
    "        descriptions[k] = v['def']\n",
    "\n",
    "pvals_sorted['description'] = pvals_sorted['index'].apply(lambda x: descriptions[x] if x in list(descriptions.keys()) else '')\n",
    "pvals_sorted['description_red'] = pvals_sorted.description.map(lambda x: \" \".join(x.split(' ')[:3] + ['...']) if len(x.split(' '))>3 else x )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-costa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pvals_sorted.sort_values(by=['fold_increase'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entertaining-judgment",
   "metadata": {},
   "outputs": [],
   "source": [
    "pvals_sorted[pvals_sorted.loc[:,'index'] == 'GO:0018580'].description_red.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-fluid",
   "metadata": {},
   "source": [
    "#### Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-girlfriend",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import (WordCloud, get_single_color_func)\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personal-theology",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dict_sub_tree = {}\n",
    "\n",
    "for term in pvals_index:\n",
    "    if graph[term][\"namespace\"] == \"biological_process\":\n",
    "        dict_sub_tree.setdefault(graph[term][\"namespace\"], []).append(pvals_sorted[pvals_sorted.loc[:,'index'] == term].description_red.values[0])\n",
    "    elif graph[term][\"namespace\"] == \"cellular_component\":\n",
    "        dict_sub_tree.setdefault(graph[term][\"namespace\"], []).append(pvals_sorted[pvals_sorted.loc[:,'index'] == term].description_red.values[0])\n",
    "    else:\n",
    "        dict_sub_tree.setdefault(graph[term][\"namespace\"], []).append(pvals_sorted[pvals_sorted.loc[:,'index'] == term].description_red.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bound-participant",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGroupedColorFunc(object):\n",
    "    \"\"\"Create a color function object which assigns EXACT colors\n",
    "       to certain words based on the color to words mapping\n",
    "\n",
    "       Parameters\n",
    "       ----------\n",
    "       color_to_words : dict(str -> list(str))\n",
    "         A dictionary that maps a color to the list of words.\n",
    "\n",
    "       default_color : str\n",
    "         Color that will be assigned to a word that's not a member\n",
    "         of any value from color_to_words.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, color_to_words, default_color):\n",
    "        self.word_to_color = {word: color\n",
    "                              for (color, words) in color_to_words.items()\n",
    "                              for word in words}\n",
    "\n",
    "        self.default_color = default_color\n",
    "\n",
    "    def __call__(self, word, **kwargs):\n",
    "        return self.word_to_color.get(word, self.default_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-courtesy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loved-value",
   "metadata": {},
   "outputs": [],
   "source": [
    "font_path = path.join('data', 'part_2', 'functional_analysis', 'Baskerville.ttf')\n",
    "\n",
    "wc = WordCloud(\n",
    "    font_path = font_path,\n",
    "    mask = np.array(Image.open(path.join('data', 'part_2', 'functional_analysis', 'mask1.png'))),\n",
    "    width = 800,\n",
    "    height = 500,\n",
    "    background_color=\"rgba(255, 255, 255, 0)\",\n",
    "    mode=\"RGBA\",\n",
    "    color_func = lambda *args, **kwargs: (18,10,143),\n",
    "    max_words = 200,\n",
    "    scale = 3,\n",
    "    prefer_horizontal = 1,\n",
    "    font_step=1,\n",
    "    min_font_size = 7,\n",
    "    max_font_size = 40\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "frequencies = pvals_sorted.set_index('description_red').fold_increase\n",
    "wc.generate_from_frequencies(frequencies)\n",
    "\n",
    "# palette 1\n",
    "# color_to_words = {\n",
    "#     '#264653': dict_sub_tree['biological_process'],\n",
    "#     '#E9C46A': dict_sub_tree['cellular_component'],\n",
    "#     '#E76F51': dict_sub_tree['molecular_function']\n",
    "# }\n",
    "\n",
    "# palette 2\n",
    "color_to_words = {\n",
    "    '#2E6F95': dict_sub_tree['biological_process'],\n",
    "    '#43AA8B': dict_sub_tree['cellular_component'],\n",
    "    '#B7094C': dict_sub_tree['molecular_function']\n",
    "}\n",
    "\n",
    "# Words that are not in any of the color_to_words values\n",
    "# will be colored with a grey single color function\n",
    "default_color = 'grey' \n",
    "\n",
    "# Create a color function with single tone\n",
    "grouped_color_func = SimpleGroupedColorFunc(color_to_words, default_color)\n",
    "\n",
    "# Apply our color function\n",
    "wc.recolor(color_func=grouped_color_func)\n",
    "\n",
    "path_wc = path.join('data', 'part_2', 'functional_analysis', 'wc.png')\n",
    "wc.to_file(path_wc)\n",
    "wc.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributed-expense",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
